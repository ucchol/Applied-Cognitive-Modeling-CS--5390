{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVVnjci15U+/FoG8ngs6nB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucchol/Applied-Cognitive-Modeling-CS--5390/blob/main/Cognitive_attributes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. define the attributes\n",
        "2. define the similarity functions\n",
        "3. define the options"
      ],
      "metadata": {
        "id": "HutZQ9WpgyJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGQa24ImedxR",
        "outputId": "f45aa4cd-c6cf-4aad-ce09-c05acc9f983c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyibl\n",
            "  Downloading pyibl-5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.9/dist-packages (from pyibl) (3.6.0)\n",
            "Collecting ordered-set\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pyibl) (23.0)\n",
            "Collecting pyactup>=2.0\n",
            "  Downloading pyactup-2.0.9-py3-none-any.whl (18 kB)\n",
            "Collecting pylru\n",
            "  Downloading pylru-1.2.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pyactup>=2.0->pyibl) (1.22.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable->pyibl) (0.2.6)\n",
            "Installing collected packages: pylru, ordered-set, pyactup, pyibl\n",
            "Successfully installed ordered-set-4.1.0 pyactup-2.0.9 pyibl-5.0 pylru-1.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyibl --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choices an agent decides between are not limited to atomic entities as we’ve used in the above. They can be structured using “attributes.” Such attributes need to be declared when the agent is created.\n",
        "\n",
        "As a concrete example, we’ll have our agent decide which of two buttons, 'left' or 'right', to push. But one of these buttons will be illuminated. Which is illuminated at any time is decided randomly, with even chances for either. Pushing the left button earns a base reward of 1, and the right button of 2; but when a button is illuminated its reward is doubled.\n",
        "\n",
        "We’ll define our agent to have two attributes, \"button\" and \"illuminatted\". The first is which button, and the second is whether or not that button is illumiunated. In this example the the \"button\" value is the decision to be made, and \"illuminatted\" value represents the context, or situation, in which this decision is being made.\n",
        "\n",
        "We’ll start by creating an agent, and two choices, one for each button."
      ],
      "metadata": {
        "id": "qIoAEXuMfA1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyibl import Agent\n",
        "from random import random\n",
        "import math\n",
        "import sys\n",
        "a = Agent([\"button\", \"illuminated\"], default_utility=5)\n",
        "left = { \"button\": \"left\", \"illuminated\": False }\n",
        "right = { \"button\": \"right\", \"illuminated\": False }"
      ],
      "metadata": {
        "id": "QyRTezR8el49"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we’ve created them both with the button un-illuminated, the code that actually runs the experiment will turn one of them on, randomly."
      ],
      "metadata": {
        "id": "l8EhSle6fDD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def push_button():\n",
        "    if random() <= 0.5:\n",
        "        left[\"illuminated\"] = True\n",
        "    else:\n",
        "        left[\"illuminated\"] = False\n",
        "    right[\"illuminated\"] = not left[\"illuminated\"]\n",
        "    result = a.choose([left, right])\n",
        "    reward = 1 if result[\"button\"] == \"left\" else 2\n",
        "    if result[\"illuminated\"]:\n",
        "        reward *= 2\n",
        "    a.respond(reward)\n",
        "    return result\n",
        "\n",
        "push_button()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgfvXpXMewT5",
        "outputId": "89e41035-0847-48fa-e2c0-720e4779c608"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'button': 'right', 'illuminated': False}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we’ll reset the agent, and then run it 2,000 times, counting how many times each button is picked, and how many times an illuminated button is picked."
      ],
      "metadata": {
        "id": "vD7xQhlMfIA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.reset()\n",
        "results = {'left': 0, 'right': 0, True: 0, False: 0}\n",
        "for _ in range(2000):\n",
        "    result = push_button()\n",
        "    results[result[\"button\"]] += 1\n",
        "    results[result[\"illuminated\"]] += 1\n",
        "\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_kXkp8Ge5nQ",
        "outputId": "a5151f26-a79d-44f3-cb20-4c469d62a71e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'left': 493, 'right': 1507, True: 1514, False: 486}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we might have expected the right button is favored, as are illuminated ones, but since an illuminated left is as good as a non-illuminated right neither completely dominates."
      ],
      "metadata": {
        "id": "A9uP9vizfN3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial matching"
      ],
      "metadata": {
        "id": "258v7i_tha28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous examples experience from prior experiences only applied if the prior decisions, or their contexts, matched exactly the ones being considered for the current choice. But often we want to choose the option that most closely matches, though not necessarily exactly, for some definition of “closely.” To do this we define a similarity function for those attributes we want to partially match, and specify a mismatch_penalty parameter.\n",
        "\n",
        "In this example there will be a continuous function, f(), that maps a number between zero and one to a reward value. At each round the model will be passed five random numbers between zero and one, and be asked to select the one that it expects will give the greatest reward. We’ll start by defining an agent that expects choices to have a single attribute, n."
      ],
      "metadata": {
        "id": "LDGrMZZShc6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = Agent([\"n\"])"
      ],
      "metadata": {
        "id": "6ChijDKjhscz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll define a similarity function for that attribute, a function of two variables, two different values of the attribute to be compared. When the attribute values are the same the value should be 1, and when they are maximally dissimilar, 0. The similarity function we’re supplying is scaled linearly, and its value ranges from 0, if one of its arguments is 1 and the other is 0, and otherwise scales up to 1 when they are equal. So, for example, 0.31 and 0.32 have a large similarity, 0.99, but 0.11 and 0.93 have a small similarity, 0.18."
      ],
      "metadata": {
        "id": "0J0gmbz0iBuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.similarity([\"n\"], lambda x, y: 1 - abs(x - y))\n",
        "print(a.similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBhdK8UMiU2U",
        "outputId": "7a156787-29da-4c6f-fdd7-094495bcd052"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Agent.similarity of <Agent agent-3 139866362101280>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mismatch_penalty is a non-negative number that says how much to penalize past experiences for poor matches to the options currently under consideration. The larger its value, the more mismatches are penalized. We’ll experiment with different values of the mismatch_penalty in our model\n",
        "\n",
        "Let’s define a function that will run our model, with the number of iterations, the mismatch_penalty, and the reward function supplied as parameters. Note that we reset the agent at the beginning of this function. We then supply one starting datum for the model to use, the value of the reward function when applied to zero. After asking the agent to choose one of five, randomly assigned values, our run_model function will work out which would have given the highest reward, and keep track of how often the model did make that choice. We’ll round that fraction of correct choices made down to two decimal places to make sure it is displayed nicely."
      ],
      "metadata": {
        "id": "Cqni7UVOjYSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(trials, mismatch, f):\n",
        "    a.reset()\n",
        "    a.mismatch_penalty = mismatch\n",
        "    a.populate([{\"n\": 0}], f(0))\n",
        "    number_correct = 0\n",
        "    fraction_correct = []\n",
        "    for t in range(trials):\n",
        "        options = [ {\"n\": random()} for _ in range(5) ]\n",
        "        choice = a.choose(options)\n",
        "        best = -sys.float_info.max\n",
        "        best_choice = None\n",
        "        for o in options:\n",
        "            v = f(o[\"n\"])\n",
        "            if o == choice:\n",
        "                a.respond(v)\n",
        "            if v > best:\n",
        "                best = v\n",
        "                best_choice = o\n",
        "        if choice == best_choice:\n",
        "            number_correct += 1\n",
        "        fraction_correct.append(float(f\"{number_correct / (t + 1):.2f}\"))\n",
        "    return fraction_correct"
      ],
      "metadata": {
        "id": "DJBqneU2jbkX"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}